{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchvision torchaudio --quiet\n",
    "! pip install transformers --quiet\n",
    "! pip install datasets --quiet\n",
    "! pip install accelerate --quiet\n",
    "! pip install sentencepiece --quiet\n",
    "! pip install peft --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajithbalakrishnan/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and modules\n",
    "import torch  # PyTorch library for tensor computations and deep learning\n",
    "import os  # Module for interacting with the operating system\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig  # Transformers library for model and processor\n",
    "from transformers.image_utils import load_image  # Utility for loading images\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import PIL\n",
    "PIL.Image.MAX_IMAGE_PIXELS = None\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the available device (GPU, MPS, or CPU) for computation\n",
    "device = (\n",
    "    \"cuda\"  # Use CUDA if available\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"  # Use Metal Performance Shaders (MPS) if available\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"  # Default to CPU if no GPU or MPS is available\n",
    ")\n",
    "\n",
    "# Configure quantization settings for efficient model loading\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load model weights in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for better accuracy\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Specify quantization type as NF4\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 for computation\n",
    ")\n",
    "\n",
    "# Define the model name to be loaded\n",
    "model_name = \"HuggingFaceTB/SmolVLM-500M-Instruct\"\n",
    "\n",
    "# Load the pre-trained model with the specified quantization configuration\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for model computations\n",
    "    _attn_implementation=\"flash_attention_2\",  # Use optimized attention implementation\n",
    ").to(device)  # Move the model to the selected device\n",
    "\n",
    "# Load the processor associated with the model for preprocessing inputs\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 800/800 [00:03<00:00, 204.34 examples/s]\n",
      "Generating validation split: 100%|██████████| 100/100 [00:00<00:00, 224.03 examples/s]\n",
      "Generating test split: 100%|██████████| 100/100 [00:00<00:00, 258.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the dataset name to be loaded\n",
    "dataset_name = 'naver-clova-ix/cord-v1'\n",
    "\n",
    "# Load the dataset using the Hugging Face datasets library\n",
    "# The dataset contains scanned document images and their corresponding ground truth labels\n",
    "ds = load_dataset(dataset_name)\n",
    "\n",
    "# Display the loaded dataset structure\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sample = ds['train'][3]\n",
    "\n",
    "sample['image_size'] = sample['image'].size,\n",
    "\n",
    "plt.imshow(sample['image'])\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Sample Chart Image\")\n",
    "plt.show()\n",
    "\n",
    "print(sample)\n",
    "\n",
    "query = 'Extract the nutritional facts from the image.'\n",
    "# Preprocess the sample\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": query}]}]\n",
    "formatted_query = processor.apply_chat_template(prompt, tokenize=False)\n",
    "\n",
    "inputs = processor(\n",
    "    images=sample[\"image\"], \n",
    "    text=formatted_query, \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "inputs = {key: val.to(device, dtype=torch.bfloat16) if val.dtype == torch.float else val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs,\n",
    "                             max_length=1600)\n",
    "\n",
    "# Decode the prediction\n",
    "prediction = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Expected Answer: {sample['ground_truth']}\")\n",
    "print(f\"Model Prediction: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gt_parse': {'menu': [{'nm': 'Bintang Bremer', 'cnt': '1', 'price': '59,000'}, {'nm': 'Chicken H-H', 'cnt': '1', 'price': '190,000'}, {'nm': 'Ades', 'cnt': '1', 'price': '10,000'}], 'sub_total': {'subtotal_price': '259,000', 'discount_price': '19,000', 'service_price': '9,600', 'tax_price': '52,416'}, 'total': {'total_price': '302,016'}}, 'meta': {'version': '1.0.0', 'split': 'train', 'image_id': 3, 'image_size': {'width': 1108, 'height': 1478}}, 'valid_line': [{'words': [{'quad': {'x2': 261, 'y3': 969, 'x3': 258, 'y4': 966, 'x1': 188, 'y1': 932, 'x4': 186, 'y2': 935}, 'is_key': 1, 'row_id': 988386, 'text': 'Sub'}, {'quad': {'x2': 353, 'y3': 967, 'x3': 352, 'y4': 966, 'x1': 265, 'y1': 932, 'x4': 264, 'y2': 931}, 'is_key': 1, 'row_id': 988386, 'text': 'Total'}, {'quad': {'x2': 851, 'y3': 967, 'x3': 851, 'y4': 968, 'x1': 704, 'y1': 925, 'x4': 705, 'y2': 926}, 'is_key': 0, 'row_id': 988386, 'text': '259,000'}], 'category': 'sub_total.subtotal_price', 'group_id': 31}, {'words': [{'quad': {'x2': 242, 'y3': 1063, 'x3': 241, 'y4': 1063, 'x1': 177, 'y1': 1027, 'x4': 176, 'y2': 1027}, 'is_key': 1, 'row_id': 988388, 'text': 'Tax'}, {'quad': {'x2': 859, 'y3': 1067, 'x3': 857, 'y4': 1065, 'x1': 728, 'y1': 1023, 'x4': 729, 'y2': 1025}, 'is_key': 0, 'row_id': 988388, 'text': '52,416'}], 'category': 'sub_total.tax_price', 'group_id': 31}, {'words': [{'quad': {'x2': 308, 'y3': 1017, 'x3': 311, 'y4': 1016, 'x1': 180, 'y1': 979, 'x4': 179, 'y2': 979}, 'is_key': 1, 'row_id': 988387, 'text': 'Service'}, {'quad': {'x2': 854, 'y3': 1018, 'x3': 852, 'y4': 1020, 'x1': 749, 'y1': 975, 'x4': 750, 'y2': 974}, 'is_key': 0, 'row_id': 988387, 'text': '9,600'}], 'category': 'sub_total.service_price', 'group_id': 31}, {'words': [{'quad': {'x2': 330, 'y3': 1110, 'x3': 328, 'y4': 1109, 'x1': 172, 'y1': 1070, 'x4': 170, 'y2': 1072}, 'is_key': 1, 'row_id': 988389, 'text': 'Discount'}, {'quad': {'x2': 861, 'y3': 1110, 'x3': 861, 'y4': 1112, 'x1': 733, 'y1': 1071, 'x4': 732, 'y2': 1068}, 'is_key': 0, 'row_id': 988389, 'text': '19,000'}], 'category': 'sub_total.discount_price', 'group_id': 31}, {'words': [{'quad': {'x2': 317, 'y3': 806, 'x3': 316, 'y4': 806, 'x1': 183, 'y1': 765, 'x4': 183, 'y2': 766}, 'is_key': 0, 'row_id': 988383, 'text': 'Bintang'}, {'quad': {'x2': 456, 'y3': 799, 'x3': 453, 'y4': 800, 'x1': 327, 'y1': 764, 'x4': 325, 'y2': 765}, 'is_key': 0, 'row_id': 988383, 'text': 'Bremer'}], 'category': 'menu.nm', 'group_id': 32}, {'words': [{'quad': {'x2': 574, 'y3': 795, 'x3': 575, 'y4': 796, 'x1': 557, 'y1': 763, 'x4': 557, 'y2': 762}, 'is_key': 0, 'row_id': 988383, 'text': '1'}], 'category': 'menu.cnt', 'group_id': 32}, {'words': [{'quad': {'x2': 844, 'y3': 799, 'x3': 845, 'y4': 800, 'x1': 721, 'y1': 760, 'x4': 722, 'y2': 758}, 'is_key': 0, 'row_id': 988383, 'text': '59,000'}], 'category': 'menu.price', 'group_id': 32}, {'words': [{'quad': {'x2': 323, 'y3': 849, 'x3': 325, 'y4': 852, 'x1': 184, 'y1': 814, 'x4': 185, 'y2': 814}, 'is_key': 0, 'row_id': 988384, 'text': 'Chicken'}, {'quad': {'x2': 401, 'y3': 849, 'x3': 404, 'y4': 850, 'x1': 334, 'y1': 814, 'x4': 334, 'y2': 812}, 'is_key': 0, 'row_id': 988384, 'text': 'H-H'}], 'category': 'menu.nm', 'group_id': 33}, {'words': [{'quad': {'x2': 576, 'y3': 844, 'x3': 577, 'y4': 845, 'x1': 556, 'y1': 811, 'x4': 561, 'y2': 808}, 'is_key': 0, 'row_id': 988384, 'text': '1'}], 'category': 'menu.cnt', 'group_id': 33}, {'words': [{'quad': {'x2': 846, 'y3': 847, 'x3': 847, 'y4': 848, 'x1': 704, 'y1': 808, 'x4': 706, 'y2': 806}, 'is_key': 0, 'row_id': 988384, 'text': '190,000'}], 'category': 'menu.price', 'group_id': 33}, {'words': [{'quad': {'x2': 278, 'y3': 898, 'x3': 280, 'y4': 901, 'x1': 190, 'y1': 862, 'x4': 187, 'y2': 861}, 'is_key': 0, 'row_id': 988385, 'text': 'Ades'}], 'category': 'menu.nm', 'group_id': 34}, {'words': [{'quad': {'x2': 579, 'y3': 890, 'x3': 579, 'y4': 891, 'x1': 556, 'y1': 858, 'x4': 556, 'y2': 857}, 'is_key': 0, 'row_id': 988385, 'text': '1'}], 'category': 'menu.cnt', 'group_id': 34}, {'words': [{'quad': {'x2': 844, 'y3': 894, 'x3': 844, 'y4': 895, 'x1': 722, 'y1': 857, 'x4': 723, 'y2': 854}, 'is_key': 0, 'row_id': 988385, 'text': '10,000'}], 'category': 'menu.price', 'group_id': 34}, {'words': [{'quad': {'x2': 335, 'y3': 1197, 'x3': 334, 'y4': 1197, 'x1': 159, 'y1': 1147, 'x4': 161, 'y2': 1149}, 'is_key': 1, 'row_id': 988390, 'text': 'TOTAL'}, {'quad': {'x2': 864, 'y3': 1199, 'x3': 864, 'y4': 1199, 'x1': 687, 'y1': 1146, 'x4': 687, 'y2': 1146}, 'is_key': 0, 'row_id': 988390, 'text': '302,016'}], 'category': 'total.total_price', 'group_id': 35}], 'roi': {'x2': 983, 'y3': 1304, 'x3': 1014, 'y4': 1309, 'x1': 119, 'y1': 234, 'x4': 91, 'y2': 228}, 'repeating_symbol': [[{'quad': {'x2': 853, 'y3': 632, 'x3': 855, 'y4': 637, 'x1': 165, 'y1': 609, 'x4': 167, 'y2': 602}, 'text': '-'}], [{'quad': {'x2': 858, 'y3': 698, 'x3': 859, 'y4': 707, 'x1': 168, 'y1': 677, 'x4': 167, 'y2': 671}, 'text': '-'}], [{'quad': {'x2': 859, 'y3': 753, 'x3': 861, 'y4': 762, 'x1': 175, 'y1': 736, 'x4': 176, 'y2': 728}, 'text': '-'}], [{'quad': {'x2': 868, 'y3': 926, 'x3': 871, 'y4': 931, 'x1': 191, 'y1': 909, 'x4': 193, 'y2': 902}, 'text': '-'}], [{'quad': {'x2': 874, 'y3': 1136, 'x3': 874, 'y4': 1139, 'x1': 166, 'y1': 1113, 'x4': 170, 'y2': 1114}, 'text': '-'}], [{'quad': {'x2': 885, 'y3': 1220, 'x3': 884, 'y4': 1221, 'x1': 161, 'y1': 1196, 'x4': 157, 'y2': 1198}, 'text': '-'}]], 'dontcare': []}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.loads((sample['ground_truth'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,114,112 || all params: 508,596,416 || trainable%: 0.2191\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to collate and preprocess a batch of examples for training\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collates and preprocesses a batch of examples for training.\n",
    "\n",
    "    Args:\n",
    "        examples (list): A list of examples, where each example is a dictionary containing:\n",
    "            - 'image': A PIL image object representing the input image.\n",
    "            - 'ground_truth': The ground truth data associated with the image.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - 'input_ids': Tokenized text inputs.\n",
    "            - 'attention_mask': Attention masks for the text inputs.\n",
    "            - 'pixel_values': Preprocessed image tensors.\n",
    "            - 'labels': Tokenized labels with padding tokens and image tokens masked out.\n",
    "    \"\"\"\n",
    "    # Define the system message for the Vision Language Model\n",
    "    system_message = \"\"\"You are a Vision Language Model specialized in interpreting visual data from a dataset containing fast-selling food product images and their details.\n",
    "    Your task is to analyze the provided image and extract structured nutritional facts such as calories, fat content, protein, and other relevant information.\n",
    "    Your responses should be concise, accurate, and relevant to the visual content of the product. Avoid additional explanation unless absolutely necessary.\"\"\"\n",
    "\n",
    "    # Initialize lists to store text and image inputs\n",
    "    text_inputs = []\n",
    "    image_inputs = []\n",
    "\n",
    "    # Iterate through each example in the batch\n",
    "    for example in examples:\n",
    "        # Format the example with system and user messages\n",
    "        formatted_example = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": query,\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "        # Apply the chat template to format the text input\n",
    "        text_inputs.append(processor.apply_chat_template(formatted_example[\"messages\"], tokenize=False).strip())\n",
    "        \n",
    "        # Ensure the image is in RGB mode\n",
    "        image = example[\"image\"]\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        image_inputs.append([image])\n",
    "\n",
    "    # Preprocess the text and image inputs using the processor\n",
    "    batch = processor(\n",
    "        text=text_inputs,\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Clone the input IDs to create labels\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    # Replace padding token IDs with -100 to ignore them during loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100 \n",
    "\n",
    "    # Get the token ID for image tokens\n",
    "    image_token_id = processor.tokenizer.convert_tokens_to_ids(str(processor.image_token))\n",
    "    # Replace image token IDs with -100 to ignore them during loss computation\n",
    "    labels[labels == image_token_id] = -100\n",
    "\n",
    "    # Add the processed labels to the batch\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-09 21:39:44,963] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557544/3033082569.py:25: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/opt/conda/envs/udop_env_v1/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/envs/udop_env_v1/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Configure the training arguments for supervised fine-tuning (SFT)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"sft_output\",  # Directory to save the fine-tuned model\n",
    "    num_train_epochs=10,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size per device during training\n",
    "    gradient_accumulation_steps=2,  # Number of steps to accumulate gradients before updating weights\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer to use for training\n",
    "    logging_steps=500,  # Log training progress every 500 steps\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    learning_rate=2e-4,  # Learning rate for the optimizer\n",
    "    bf16=True,  # Use bfloat16 precision for training\n",
    "    tf32=True,  # Enable TensorFloat-32 precision for faster computation on supported hardware\n",
    "    max_grad_norm=0.3,  # Maximum gradient norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Warmup ratio for the learning rate scheduler\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Additional kwargs for gradient checkpointing\n",
    "    dataloader_num_workers=4,  # Number of workers for data loading\n",
    "    dataset_text_field=\"\",  # Placeholder for dataset text field (not used in this case)\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Skip dataset preparation step\n",
    "    remove_unused_columns=False  # Do not remove unused columns from the dataset\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer for supervised fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Pre-trained model to fine-tune\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=ds[\"train\"],  # Training dataset\n",
    "    eval_dataset=ds[\"test\"],  # Evaluation dataset\n",
    "    data_collator=collate_fn,  # Function to collate and preprocess batches of data\n",
    "    peft_config=peft_config,  # PEFT (Parameter-Efficient Fine-Tuning) configuration\n",
    "    tokenizer=processor.tokenizer,  # Tokenizer associated with the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3109' max='12740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3109/12740 8:19:43 < 25:49:02, 0.10 it/s, Epoch 2.44/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./smolvlm_instruct_finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/udop_env_v1/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/udop_env_v1/lib/python3.12/site-packages/transformers/trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train() # Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./smolvlm_instruct_finetuned\") # Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"./smolvlm_instruct_finetuned\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\",\n",
    ").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"./smolvlm_instruct_finetuned\") # Load the processor\n",
    "# Load the test dataset\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\") # Load the test dataset\n",
    "# Define a function to evaluate the model on the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and returns the predictions and ground truth labels.\n",
    "\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    # Iterate through each example in the test dataset\n",
    "    for example in test_dataset:\n",
    "        # Preprocess the image and query\n",
    "        prompt = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": query}]}]\n",
    "        formatted_query = processor.apply_chat_template(prompt, tokenize=False)\n",
    "        inputs = processor(\n",
    "            images=example[\"image\"],\n",
    "            text=formatted_query,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        inputs = {key: val.to(device, dtype=torch.bfloat16) if val.dtype == torch.float else val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_length=1600)\n",
    "\n",
    "        # Decode the prediction\n",
    "        prediction = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Append the prediction and ground truth to their respective lists\n",
    "        predictions.append(prediction)\n",
    "        ground_truths.append(example[\"ground_truth\"])\n",
    "\n",
    "    return predictions, ground_truths\n",
    "# Evaluate the model on the test dataset\n",
    "predictions, ground_truths = evaluate_model(model, processor, test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from jiwer import wer, cer\n",
    "import json\n",
    "\n",
    "def generate_evaluation_metrics(predictions, ground_truths):\n",
    "\n",
    "    # Flatten the predictions and ground truths if they are nested\n",
    "    flat_predictions = [pred.strip() for pred in predictions]\n",
    "    flat_ground_truths = [json.loads(gt)[\"gt_parse\"] for gt in ground_truths]\n",
    "\n",
    "    # Calculate precision, recall, F1-score, and accuracy\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(flat_ground_truths, flat_predictions, average='weighted')\n",
    "    accuracy = accuracy_score(flat_ground_truths, flat_predictions)\n",
    "\n",
    "    # Calculate Word Error Rate (WER) and Character Error Rate (CER)\n",
    "    wer_score = wer(flat_ground_truths, flat_predictions)\n",
    "    cer_score = cer(flat_ground_truths, flat_predictions)\n",
    "\n",
    "    # Combine all metrics into a dictionary\n",
    "    metrics = {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Word Error Rate (WER)\": wer_score,\n",
    "        \"Character Error Rate (CER)\": cer_score\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Generate evaluation metrics\n",
    "evaluation_metrics = generate_evaluation_metrics(predictions, ground_truths)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for metric, value in evaluation_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
